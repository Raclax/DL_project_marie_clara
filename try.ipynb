{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db017b04",
   "metadata": {},
   "source": [
    "# Project detection math expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddb947",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3483e785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2025-12-04 12:52:20,639 _showwarnmsg:109] /home/raclax/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2025-12-04 12:52:20,639 _showwarnmsg:109] /home/raclax/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix2text is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7970861ab250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, utils\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models.detection as detection\n",
    "# Install pix2text robustly using the current Python executable\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import pix2text\n",
    "except Exception:\n",
    "    print('pix2text not found â€” installing via python -m pip')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pix2text>=1.1'])\n",
    "    import pix2text\n",
    "print('pix2text is available')\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce25700",
   "metadata": {},
   "source": [
    "## Pre treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55a82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROHMEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour les expressions complÃ¨tes (PNG + LG).\n",
    "    Chaque sample retourne :\n",
    "        - image : Tensor CxHxW\n",
    "        - target : dict contenant \"boxes\" et \"labels\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, meta_classes=True):\n",
    "        \"\"\"\n",
    "        root : chemin du dossier contenant les PNG + LG\n",
    "        transform : transform PyTorch (augmentations, ToTensor, Resizeâ€¦)\n",
    "        meta_classes : si True, map chaque label vers une mÃ©ta-classe\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.meta_classes = meta_classes\n",
    "\n",
    "        # liste des fichiers PNG / LG\n",
    "        self.images = [f for f in os.listdir(root) if f.endswith(\".png\")]\n",
    "        self.images.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        lg_name = img_name.replace(\".png\", \".lg\")\n",
    "        lg_path = os.path.join(self.root, lg_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        with open(lg_path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                parts = [p.strip() for p in line.strip().split(\",\") if p.strip() != \"\"]\n",
    "                if len(parts) < 6:\n",
    "                    # fallback to whitespace splitting if commas are not reliable\n",
    "                    parts = [p.strip() for p in line.strip().split() if p.strip() != \"\"]\n",
    "\n",
    "                if len(parts) < 6:\n",
    "                    continue\n",
    "\n",
    "                label = parts[1]\n",
    "                try:\n",
    "                    xmin_s, ymin_s, xmax_s, ymax_s = parts[-4:]\n",
    "                    xmin = float(xmin_s)\n",
    "                    ymin = float(ymin_s)\n",
    "                    xmax = float(xmax_s)\n",
    "                    ymax = float(ymax_s)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                if xmax <= xmin or ymax <= ymin:\n",
    "                    warnings.warn(\n",
    "                            f\"Found invalid bbox in '{lg_path}': [xmin={xmin}, ymin={ymin}, xmax={xmax}, ymax={ymax}]. These boxes will be skipped.\")\n",
    "                    continue\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(self.map_label(label))\n",
    "\n",
    "        # Convert to tensors; ensure correct shapes even when empty\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def map_label(self, label):\n",
    "        raw = label.split(\"_\")[0].strip()\n",
    "        if raw.isalpha():\n",
    "            return 0\n",
    "\n",
    "        if raw.isdigit():\n",
    "            return 1\n",
    "\n",
    "        if raw in {\"+\", \"-\", \"=\", \"/\", \"*\", \"Ã—\", \"Ã·\", \"^\"}:\n",
    "            return 2\n",
    "        return 3\n",
    "\n",
    "    def raw_label_to_id(self, raw):\n",
    "        if not hasattr(self, \"raw_vocab\"):\n",
    "            self.raw_vocab = {}\n",
    "        if raw not in self.raw_vocab:\n",
    "            self.raw_vocab[raw] = len(self.raw_vocab)\n",
    "        return self.raw_vocab[raw]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2078b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../datas/FullExpressions/CROHME2019_train_png/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbad719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image :  torch.Size([3, 119, 500])\n",
      "Target :  {'boxes': tensor([[ 10.,  40.,  39., 108.],\n",
      "        [270.,  46., 320.,  72.],\n",
      "        [340.,  47., 371.,  71.],\n",
      "        [467.,  10., 489.,  38.],\n",
      "        [121.,  38., 166.,  63.],\n",
      "        [226.,  21., 265.,  65.],\n",
      "        [399.,  10., 452.,  76.]]), 'labels': tensor([0, 0, 2, 1, 2, 0, 0])}\n",
      "Dataset sizes -> total: 9993, train: 7994, val: 999, test: 1000\n"
     ]
    }
   ],
   "source": [
    "dataset = CROHMEDataset(\n",
    "    root=root,\n",
    "    transform=transforms.ToTensor(),\n",
    "    meta_classes=True\n",
    ")\n",
    "\n",
    "image, target = dataset[0]\n",
    "print(\"Image : \", image.size())\n",
    "print(\"Target : \", target)\n",
    "\n",
    "dataset_len = len(dataset)\n",
    "train_len = int(0.8 * dataset_len)\n",
    "val_len = int(0.1 * dataset_len)\n",
    "test_len = dataset_len - train_len - val_len\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(\n",
    "    dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes -> total: {dataset_len}, train: {train_len}, val: {val_len}, test: {test_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8164",
   "metadata": {},
   "source": [
    "## Functions for visualization and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "234f693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from file.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def prepare_image(image, transform=None):\n",
    "    \"\"\"Prepare the image for model input.\"\"\"\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    return image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def visualize_predictions(image, boxes, labels, scores, threshold=0.4):\n",
    "    \"\"\"Visualize the bounding boxes and labels on the image.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "\n",
    "    # Filter out boxes and labels below the threshold\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                                fill=False, edgecolor='red', linewidth=3))\n",
    "            plt.text(x_min, y_min, f'{label.item()}: {score:.2f}', fontsize=12, color='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4840a57",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a580f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone frozen. Only the RPN and heads will be trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  27%|â–ˆâ–ˆâ–‹       | 709/2665 [01:48<04:58,  6.55it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datas/FullExpressions/CROHME2019_train_png/128_em_525.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m model.train()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[32m     77\u001b[39m nb_used_sample = \u001b[32m0\u001b[39m \u001b[38;5;66;03m# Initialize the number of samples used in this epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move images and targets to the device (GPU or CPU)\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mCROHMEDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     30\u001b[39m lg_name = img_name.replace(\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.lg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m lg_path = os.path.join(\u001b[38;5;28mself\u001b[39m.root, lg_name)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m boxes = []\n\u001b[32m     36\u001b[39m labels = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/M2/Part2/DL2/Project/deepl/lib/python3.11/site-packages/PIL/Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../datas/FullExpressions/CROHME2019_train_png/128_em_525.png'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate =0.0008\n",
    "batch_size = 3\n",
    "# Keep val_size if you want an absolute val count fallback, but we'll use dynamic splits\n",
    "val_size = 10\n",
    "\n",
    "val_err_array = np.array([])\n",
    "train_err_array = np.array([])\n",
    "nb_sample_array = np.array([])\n",
    "train_loss_classifier_array = np.array([])\n",
    "train_loss_objectness_array = np.array([])\n",
    "\n",
    "# Early stopping parameters\n",
    "patience =5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Use the Subset objects created earlier by random_split: `train`, `val`, `test`.\n",
    "# If `train` or `val` don't exist yet (cell not executed), compute splits here as a fallback.\n",
    "try:\n",
    "    train_subset = train\n",
    "    val_subset = val\n",
    "except NameError:\n",
    "    dataset_len = len(dataset)\n",
    "    train_len = int(0.8 * dataset_len)\n",
    "    val_len = int(0.1 * dataset_len)\n",
    "    test_len = dataset_len - train_len - val_len\n",
    "    train_subset, val_subset, _ = torch.utils.data.random_split(dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Load a pretrained Faster R-CNN model\n",
    "#model = detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "#model = detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "model = detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Set the requires_grad attribute of all the backbone parameters to False\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Backbone frozen. Only the RPN and heads will be trained.\")\n",
    "\n",
    "# Modify the model for the number of classes\n",
    "num_classes = 5  # 20 classes + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Function for validation\n",
    "def validate(model, val_loader):\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')  # Initialize best validation loss\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_loss_classifier = 0.0\n",
    "    epoch_loss_objectness = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    nb_used_sample = 0 # Initialize the number of samples used in this epoch\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move images and targets to the device (GPU or CPU)\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Compute total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_loss += losses.item()\n",
    "        # Use .get to avoid KeyError if a particular loss term is missing\n",
    "        epoch_loss_classifier += loss_dict.get('loss_classifier', torch.tensor(0.0)).item()\n",
    "        epoch_loss_objectness += loss_dict.get('loss_objectness', torch.tensor(0.0)).item()\n",
    "        nb_used_sample += len(images)\n",
    "\n",
    "\n",
    "################ FOR VGG16 ###############\n",
    "    #     # Accumulate loss - Use keys appropriate for SSD\n",
    "    #     epoch_loss += losses.item()\n",
    "    #     # Assuming loss_dict for SSD contains 'classification' and 'bbox_regression'\n",
    "    #     if 'classification' in loss_dict:\n",
    "    #         epoch_loss_classifier += loss_dict['classification'].item()\n",
    "    #     if 'bbox_regression' in loss_dict:\n",
    "    #          epoch_loss_objectness += loss_dict['bbox_regression'].item() # Using objectness for regression loss here\n",
    "    #     nb_used_sample += batch_size\n",
    "\n",
    "\n",
    "    # # Calculate average training loss for the epoch\n",
    "    # train_err = epoch_loss / len(train_loader)\n",
    "    # # Calculate average for classifier and regression losses only if they were accumulated\n",
    "    # train_loss_classifier = epoch_loss_classifier / len(train_loader) if 'classification' in loss_dict else 0\n",
    "    # train_loss_objectness = epoch_loss_objectness / len(train_loader) if 'bbox_regression' in loss_dict else 0\n",
    "###########################################\n",
    "\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    train_err = epoch_loss / len(train_loader)\n",
    "    train_loss_classifier = epoch_loss_classifier / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "    train_loss_objectness = epoch_loss_objectness / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_err:.4f}, Classifier Loss: {train_loss_classifier:.4f}, Objectness Loss: {train_loss_objectness:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    val_loss = validate(model, val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    train_err_array = np.append(train_err_array, train_err)\n",
    "    val_err_array = np.append(val_err_array, val_loss)\n",
    "    nb_sample_array = np.append(nb_sample_array, nb_used_sample)\n",
    "    train_loss_classifier_array = np.append(train_loss_classifier_array, train_loss_classifier)\n",
    "    train_loss_objectness_array = np.append(train_loss_objectness_array, train_loss_objectness)\n",
    "\n",
    "    # Save the model weights if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'faster_rcnn_voc_best.pth')\n",
    "        print(f\"Model weights saved. New best validation loss: {best_val_loss:.4f}\")\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping after {patience} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "# Final message\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c458ea3",
   "metadata": {},
   "source": [
    "## Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90066be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single image for inference\n",
    "image_path = full_train_dataset.image_dir + '/2007_000423.jpg'  # Replace with your image path\n",
    "image = load_image(image_path)\n",
    "\n",
    "model.load_state_dict(torch.load('faster_rcnn_voc_best.pth', weights_only=True))\n",
    "\n",
    "print(image)\n",
    "# Prepare the model for inference\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Prepare the image\n",
    "    input_image_ = prepare_image(image, transform)\n",
    "    input_image = input_image_.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    predictions = model(input_image)\n",
    "\n",
    "# Extract boxes, labels, and scores from predictions\n",
    "boxes = predictions[0]['boxes']\n",
    "labels = predictions[0]['labels']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "boxes = boxes.to('cpu')\n",
    "labels = labels.to('cpu')\n",
    "scores = scores.to('cpu')\n",
    "\n",
    "print(boxes)\n",
    "print(labels)\n",
    "print(scores)\n",
    "\n",
    "# Visualize the results\n",
    "visualize_predictions(input_image_[0], boxes, labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d1ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU et mAP qui viennent d'Object_Segmentation\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    # Slicing idx:idx+1 in order to keep tensor dimensionality\n",
    "    # Doing ... in indexing if there would be additional dimensions\n",
    "    # Like for Yolo algorithm which would have (N, S, S, 4) in shape\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Need clamp(0) in case they do not intersect, then we want intersection to be 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"corners\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_ground_truths(model, val_loader, device):\n",
    "    model.eval()\n",
    "    pred_boxes = []  # To store predictions\n",
    "    true_boxes = []  # To store ground truths\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                # Get the predicted boxes, scores, and labels\n",
    "                pred_boxes_list = output['boxes'].cpu().numpy()\n",
    "                pred_scores_list = output['scores'].cpu().numpy()\n",
    "                pred_labels_list = output['labels'].cpu().numpy()\n",
    "\n",
    "                # Filter out predictions with low scores\n",
    "                for j in range(len(pred_boxes_list)):\n",
    "                    if pred_scores_list[j] >= 0.05:  # Score threshold\n",
    "                        pred_boxes.append([\n",
    "                            i,  # image index\n",
    "                            pred_labels_list[j],\n",
    "                            pred_scores_list[j],\n",
    "                            *pred_boxes_list[j]\n",
    "                        ])\n",
    "\n",
    "                # Get ground truth boxes and labels\n",
    "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "                gt_labels = targets[i]['labels'].cpu().numpy()\n",
    "\n",
    "                for k in range(len(gt_boxes)):\n",
    "                    true_boxes.append([\n",
    "                        i,  # image index\n",
    "                        gt_labels[k],\n",
    "                        1.0,  # Assuming ground truth boxes have a score of 1.0\n",
    "                        *gt_boxes[k]\n",
    "                    ])\n",
    "\n",
    "    return pred_boxes, true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes, true_boxes = get_predictions_and_ground_truths(model, val_loader, device)\n",
    "mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"corners\", num_classes=num_classes)\n",
    "print(f\"Mean Average Precision (mAP): {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c33ac1",
   "metadata": {},
   "source": [
    "# YOLO v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a5921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.235 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=math_symbols_detector12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=math_symbols_detector12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,137,148 parameters, 11,137,132 gradients, 28.7 GFLOPs\n",
      "\n",
      "Model summary: 129 layers, 11,137,148 parameters, 11,137,132 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 15.2Â±9.9 MB/s, size: 1.9 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 15.2Â±9.9 MB/s, size: 1.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/train.cache... 7993 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 7993/7993 6.3Mit/s 0.0s0s\n",
      "\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 13.4Â±6.9 MB/s, size: 2.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 13.4Â±6.9 MB/s, size: 2.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.5Mit/s 0.0ss\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.5Mit/s 0.0ss\n",
      "Plotting labels to /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/labels.jpg... \n",
      "Plotting labels to /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10     0.791G      1.164      1.529     0.9369         99        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 6.8it/s 1:14<0.2ss\n",
      "\u001b[K       1/10     0.791G      1.164      1.529     0.9369         99        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 6.8it/s 1:14<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 4.2it/s 14.9s0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 4.2it/s 14.9s\n",
      "                   all       1999      19394      0.517       0.52      0.479      0.309\n",
      "                   all       1999      19394      0.517       0.52      0.479      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      1.09G     0.9978      1.129     0.8911         63        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.2it/s 1:10<0.1ss\n",
      "\u001b[K       2/10      1.09G     0.9978      1.129     0.8911         63        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.2it/s 1:10<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.6it/s 11.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.6it/s 11.3s\n",
      "                   all       1999      19394      0.605      0.553      0.559      0.374\n",
      "                   all       1999      19394      0.605      0.553      0.559      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      1.09G     0.9494      1.045     0.8823        114        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.8it/s 1:04<0.1ss\n",
      "\u001b[K       3/10      1.09G     0.9494      1.045     0.8823        114        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.8it/s 1:04<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.7it/s 11.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.7it/s 11.1s\n",
      "                   all       1999      19394      0.659      0.584      0.602      0.418\n",
      "                   all       1999      19394      0.659      0.584      0.602      0.418\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      1.09G     0.8915     0.9599     0.8676         66        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1ss\n",
      "\u001b[K       4/10      1.09G     0.8915     0.9599     0.8676         66        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.5it/s 11.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.5it/s 11.5s\n",
      "                   all       1999      19394      0.694      0.601      0.636      0.431\n",
      "                   all       1999      19394      0.694      0.601      0.636      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      1.09G     0.8324     0.8896     0.8546         69        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 8.0it/s 1:03<0.1ss\n",
      "\u001b[K       5/10      1.09G     0.8324     0.8896     0.8546         69        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 8.0it/s 1:03<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.8it/s 10.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.8it/s 10.8s\n",
      "                   all       1999      19394      0.686      0.605      0.635      0.451\n",
      "                   all       1999      19394      0.686      0.605      0.635      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      1.09G      0.787     0.8253     0.8487         84        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1ss\n",
      "\u001b[K       6/10      1.09G      0.787     0.8253     0.8487         84        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.3it/s 12.0s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.3it/s 12.0s\n",
      "                   all       1999      19394      0.743      0.643      0.698      0.493\n",
      "                   all       1999      19394      0.743      0.643      0.698      0.493\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      1.09G     0.7458     0.7713     0.8411         84        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:05<0.1ss\n",
      "\u001b[K       7/10      1.09G     0.7458     0.7713     0.8411         84        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:05<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.4it/s 11.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.4it/s 11.6s\n",
      "                   all       1999      19394      0.764      0.662      0.718      0.533\n",
      "                   all       1999      19394      0.764      0.662      0.718      0.533\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      1.12G     0.7087     0.7199     0.8372         92        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1ss\n",
      "\u001b[K       8/10      1.12G     0.7087     0.7199     0.8372         92        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.6it/s 1:06<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.4it/s 11.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.4it/s 11.6s\n",
      "                   all       1999      19394      0.779      0.668      0.735      0.547\n",
      "                   all       1999      19394      0.779      0.668      0.735      0.547\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      1.12G     0.6733     0.6664     0.8291         74        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 8.0it/s 1:02<0.1ss\n",
      "\u001b[K       9/10      1.12G     0.6733     0.6664     0.8291         74        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 8.0it/s 1:02<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.6it/s 11.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.6it/s 11.3s\n",
      "                   all       1999      19394      0.772      0.673       0.73      0.566\n",
      "                   all       1999      19394      0.772      0.673       0.73      0.566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      1.12G     0.6463     0.6207     0.8246         85        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.5it/s 1:06<0.1ss\n",
      "\u001b[K      10/10      1.12G     0.6463     0.6207     0.8246         85        256: 100% â”â”â”â”â”â”â”â”â”â”â”â” 500/500 7.5it/s 1:06<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.3it/s 11.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 5.3it/s 11.8s\n",
      "                   all       1999      19394      0.793      0.678      0.749      0.584\n",
      "                   all       1999      19394      0.793      0.678      0.749      0.584\n",
      "\n",
      "10 epochs completed in 0.220 hours.\n",
      "\n",
      "10 epochs completed in 0.220 hours.\n",
      "Optimizer stripped from /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/best.pt...\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "Optimizer stripped from /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12/weights/best.pt...\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n",
      "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 4.2it/s 15.0s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 4.2it/s 15.0s\n",
      "                   all       1999      19394      0.793      0.677      0.749      0.584\n",
      "                letter       1183       5242      0.787      0.743      0.816      0.648\n",
      "                 digit       1087       4518       0.75      0.669      0.725      0.548\n",
      "              operator       1154       2721      0.875      0.568      0.667      0.486\n",
      "                 other       1670       6913       0.76       0.73      0.789      0.655\n",
      "Speed: 0.0ms preprocess, 1.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12\u001b[0m\n",
      "                   all       1999      19394      0.793      0.677      0.749      0.584\n",
      "                letter       1183       5242      0.787      0.743      0.816      0.648\n",
      "                 digit       1087       4518       0.75      0.669      0.725      0.548\n",
      "              operator       1154       2721      0.875      0.568      0.667      0.486\n",
      "                 other       1670       6913       0.76       0.73      0.789      0.655\n",
      "Speed: 0.0ms preprocess, 1.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/math_symbols_detector12\u001b[0m\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n",
      "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 45.9Â±24.6 MB/s, size: 1.9 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 45.9Â±24.6 MB/s, size: 1.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.6Mit/s 0.0s0s\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.6Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 125/125 7.6it/s 16.5s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 125/125 7.6it/s 16.5s\n",
      "                   all       1999      19394      0.793      0.677      0.749      0.585\n",
      "                letter       1183       5242      0.787      0.742      0.816      0.648\n",
      "                 digit       1087       4518       0.75      0.669      0.725       0.55\n",
      "              operator       1154       2721      0.876      0.567      0.668      0.486\n",
      "                 other       1670       6913      0.757      0.731      0.789      0.655\n",
      "Speed: 0.1ms preprocess, 2.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/val3\u001b[0m\n",
      "                   all       1999      19394      0.793      0.677      0.749      0.585\n",
      "                letter       1183       5242      0.787      0.742      0.816      0.648\n",
      "                 digit       1087       4518       0.75      0.669      0.725       0.55\n",
      "              operator       1154       2721      0.876      0.567      0.668      0.486\n",
      "                 other       1670       6913      0.757      0.731      0.789      0.655\n",
      "Speed: 0.1ms preprocess, 2.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/runs/detect/val3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Charger un modÃ¨le prÃ©-entraÃ®nÃ©\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# EntraÃ®ner\n",
    "results = model.train(\n",
    "    data='YOLO_dataset/data.yaml',\n",
    "    epochs=10,\n",
    "    imgsz=256,\n",
    "    batch=16,\n",
    "    name='math_symbols_detector'\n",
    ")\n",
    "\n",
    "results = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb75d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict('YOLO_dataset/images/train/001-equation002.png', save=True, conf=0.5)\n",
    "result = results[0]\n",
    "\n",
    "boxes = result.boxes\n",
    "print(\"Boxes (xyxy):\", boxes.xyxy)\n",
    "print(\"Boxes (xywhn):\", boxes.xywhn)\n",
    "print(\"Confidence:\", boxes.conf)\n",
    "print(\"Classes:\", boxes.cls)\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    xyxy = box.xyxy[0].tolist()\n",
    "    conf = box.conf[0].item() \n",
    "    cls_id = int(box.cls[0].item()) \n",
    "    cls_name = result.names[cls_id] \n",
    "    \n",
    "    print(f\"Box {i}: {cls_name} (conf: {conf:.2f}) -> [{xyxy[0]:.1f}, {xyxy[1]:.1f}, {xyxy[2]:.1f}, {xyxy[3]:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc73e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 35.1Â±23.3 MB/s, size: 1.8 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 35.1Â±23.3 MB/s, size: 1.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.8Mit/s 0.0s0s\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.8Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 125/125 7.4it/s 16.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 125/125 7.4it/s 16.9s\n",
      "                   all       1999      19394      0.696      0.695      0.705      0.469\n",
      "                letter       1183       5242       0.68      0.804      0.774      0.539\n",
      "                 digit       1087       4518      0.745      0.679      0.724      0.482\n",
      "              operator       1154       2721      0.802      0.669      0.762      0.476\n",
      "                 other       1670       6913      0.555      0.628       0.56      0.379\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "                   all       1999      19394      0.696      0.695      0.705      0.469\n",
      "                letter       1183       5242       0.68      0.804      0.774      0.539\n",
      "                 digit       1087       4518      0.745      0.679      0.724      0.482\n",
      "              operator       1154       2721      0.802      0.669      0.762      0.476\n",
      "                 other       1670       6913      0.555      0.628       0.56      0.379\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "=== RÃ‰SULTATS D'Ã‰VALUATION SIMPLIFIÃ‰E ===\n",
      "mAP@0.5:  0.70518092058381\n",
      "mAP@0.5:0.95:  0.46925929764128327\n",
      "PrÃ©cision (P): [    0.68015     0.74525     0.80176     0.55488]\n",
      "Rappel (R): [    0.80446     0.67859      0.6685     0.62795]\n",
      "=== RÃ‰SULTATS D'Ã‰VALUATION SIMPLIFIÃ‰E ===\n",
      "mAP@0.5:  0.70518092058381\n",
      "mAP@0.5:0.95:  0.46925929764128327\n",
      "PrÃ©cision (P): [    0.68015     0.74525     0.80176     0.55488]\n",
      "Rappel (R): [    0.80446     0.67859      0.6685     0.62795]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def evaluate_yolo_model_simple(model, data_yaml_path, imgsz=640, conf=0.25):\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path,\n",
    "        imgsz=imgsz,\n",
    "        conf=conf,\n",
    "        split='val',\n",
    "        plots=False \n",
    "    )\n",
    "    \n",
    "\n",
    "    results = {\n",
    "        'mAP50': metrics.box.map50,       # mAP Ã  IoU=0.5\n",
    "        'mAP50-95': metrics.box.map,      # mAP moyenne sur 0.5:0.95\n",
    "        'precision': metrics.box.p,       # PrÃ©cision globale\n",
    "        'recall': metrics.box.r,          # Rappel global\n",
    "        'fitness': metrics.box.fitness    # Score de fitness\n",
    "    }\n",
    "    \n",
    "    print('=== RÃ‰SULTATS D\\'Ã‰VALUATION SIMPLIFIÃ‰E ===')\n",
    "    print(\"mAP@0.5: \",results['mAP50'])\n",
    "    print(\"mAP@0.5:0.95: \",results['mAP50-95'])\n",
    "    print(\"PrÃ©cision (P):\",results['precision'])\n",
    "    print(\"Rappel (R):\",results['recall'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "data_config = 'YOLO_dataset/data.yaml' \n",
    "\n",
    "metrics = evaluate_yolo_model_simple(model, data_config, conf=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a7d43",
   "metadata": {},
   "source": [
    "# RTDETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "\n",
    "model = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='YOLO_dataset/data.yaml',\n",
    "    epochs=5,\n",
    "    imgsz=640,\n",
    "    batch=4,\n",
    "    name='math_rtdetr'\n",
    ")\n",
    "\n",
    "metrics = model.val()\n",
    "\n",
    "predictions = model.predict('YOLO_dataset/images/001-equation001.png', conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict('YOLO_dataset/images/train/001-equation002.png', save=True, conf=0.5)\n",
    "# AccÃ©der au premier rÃ©sultat\n",
    "result = results[0]\n",
    "\n",
    "# ===== BOUNDING BOXES =====\n",
    "boxes = result.boxes\n",
    "\n",
    "print(\"Boxes (xyxy):\", boxes.xyxy)\n",
    "print(\"Boxes (xywhn):\", boxes.xywhn)\n",
    "\n",
    "# Confiance de chaque prÃ©diction\n",
    "print(\"Confidence:\", boxes.conf)\n",
    "\n",
    "print(\"Classes:\", boxes.cls)\n",
    "\n",
    "# ===== DÃ‰TAILS COMPLETS =====\n",
    "for i, box in enumerate(boxes):\n",
    "    xyxy = box.xyxy[0].tolist()\n",
    "    conf = box.conf[0].item() \n",
    "    cls_id = int(box.cls[0].item()) \n",
    "    cls_name = result.names[cls_id] \n",
    "    \n",
    "    print(f\"Box {i}: {cls_name} (conf: {conf:.2f}) -> [{xyxy[0]:.1f}, {xyxy[1]:.1f}, {xyxy[2]:.1f}, {xyxy[3]:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c1fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.234 ðŸš€ Python-3.11.5 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3769MiB)\n",
      "rt-detr-l summary: 294 layers, 32,148,140 parameters, 0 gradients, 103.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 97.2Â±40.9 MB/s, size: 2.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/raclax/Documents/M2/Part2/DL2/Project/DL_project_marie_clara/YOLO_dataset/labels/val.cache... 1999 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1999/1999 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 125/125 1.7it/s 1:140.6sss\n",
      "                   all       1999      19394      0.119    0.00122     0.0605      0.024\n",
      "                person       1183       5242      0.107    0.00229     0.0543     0.0177\n",
      "               bicycle       1087       4518          0          0          0          0\n",
      "                   car       1154       2721      0.368    0.00257      0.188     0.0785\n",
      "            motorcycle       1670       6913          0          0          0          0\n",
      "Speed: 0.8ms preprocess, 33.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "=== RÃ‰SULTATS D'Ã‰VALUATION SIMPLIFIÃ‰E ===\n",
      "mAP@0.5:  0.06053750940688701\n",
      "mAP@0.5:0.95:  0.0240442473171953\n",
      "PrÃ©cision (P): [    0.10714           0     0.36842           0]\n",
      "Rappel (R): [  0.0022892           0   0.0025726           0]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import RTDETR\n",
    "model = RTDETR('rtdetr-l.pt') \n",
    "\n",
    "data_config = 'YOLO_dataset/data.yaml' \n",
    "\n",
    "metrics = evaluate_yolo_model_simple(model, data_config, conf=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e02ad",
   "metadata": {},
   "source": [
    "# Pix2text : bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f962bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pix2text import Pix2Text\n",
    "\n",
    "# Initialize Pix2Text: try GPU first, fall back to CPU if ONNX Runtime doesn't expose CUDAExecutionProvider\n",
    "try:\n",
    "    p2t = Pix2Text.from_config(device='cuda')  # attempt GPU/ONNXRuntime CUDAExecutionProvider\n",
    "    print('Pix2Text initialized on CUDA device')\n",
    "except Exception as e:\n",
    "    # Common failure: onnxruntime not built with GPU support -> ValueError about CUDAExecutionProvider\n",
    "    print('GPU initialization failed (will fall back to CPU):', e)\n",
    "    print('Initializing Pix2Text on CPU...')\n",
    "    p2t = Pix2Text.from_config(device='cpu')\n",
    "    print('Pix2Text initialized on CPU')\n",
    "\n",
    "img_path = '../datas/FullExpressions/CROHME2019_train_png/001-equation000.png'\n",
    "try:\n",
    "    res = p2t.recognize_formula(img_path, return_text=True)\n",
    "    print(res)\n",
    "except Exception as e:\n",
    "    print('Error running recognition:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
