{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483e785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7eae1420d4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROHMEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour les expressions complètes (PNG + LG).\n",
    "    Chaque sample retourne :\n",
    "        - image : Tensor CxHxW\n",
    "        - target : dict contenant \"boxes\" et \"labels\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, meta_classes=True):\n",
    "        \"\"\"\n",
    "        root : chemin du dossier contenant les PNG + LG\n",
    "        transform : transform PyTorch (augmentations, ToTensor, Resize…)\n",
    "        meta_classes : si True, map chaque label vers une méta-classe\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.meta_classes = meta_classes\n",
    "\n",
    "        # liste des fichiers PNG / LG\n",
    "        self.images = [f for f in os.listdir(root) if f.endswith(\".png\")]\n",
    "        self.images.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        lg_name = img_name.replace(\".png\", \".lg\")\n",
    "        lg_path = os.path.join(self.root, lg_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        with open(lg_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if not line.startswith(\"BB\"):\n",
    "                    continue\n",
    "                parts = line.strip().split(\",\")\n",
    "                _, label, xmin, ymin, xmax, ymax = parts\n",
    "\n",
    "                xmin = float(xmin)\n",
    "                ymin = float(ymin)\n",
    "                xmax = float(xmax)\n",
    "                ymax = float(ymax)\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(self.map_label(label))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def map_label(self, label):\n",
    "        raw = label.split(\"_\")[0].strip()\n",
    "        if raw.isalpha():\n",
    "            return 0 \n",
    "\n",
    "        if raw.isdigit():\n",
    "            return 1 \n",
    "\n",
    "        if raw in {\"+\", \"-\", \"=\", \"/\", \"*\", \"×\", \"÷\", \"^\"}:\n",
    "            return 2\n",
    "        return 3\n",
    "\n",
    "    def raw_label_to_id(self, raw):\n",
    "        \"\"\"\n",
    "        Si tu veux travailler sans méta-classes, il faut garder un vocabulaire\n",
    "        complet → à adapter selon ton dataset.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"raw_vocab\"):\n",
    "            self.raw_vocab = {}\n",
    "        if raw not in self.raw_vocab:\n",
    "            self.raw_vocab[raw] = len(self.raw_vocab)\n",
    "        return self.raw_vocab[raw]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CROHMEDataset(\n",
    "    root=\"datas/FullExpressions/CROHME2019_train_png/\",\n",
    "    transform=transforms.ToTensor(),\n",
    "    meta_classes=True\n",
    ")\n",
    "\n",
    "image, target = dataset[0]\n",
    "print (\"Image : \", image.size())\n",
    "print (\"Target : \", target)\n",
    "train, val, test = torch.utils.data.random_split(dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from file.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def prepare_image(image, transform=None):\n",
    "    \"\"\"Prepare the image for model input.\"\"\"\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    return image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def visualize_predictions(image, boxes, labels, scores, threshold=0.4):\n",
    "    \"\"\"Visualize the bounding boxes and labels on the image.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "\n",
    "    # Filter out boxes and labels below the threshold\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                                fill=False, edgecolor='red', linewidth=3))\n",
    "            plt.text(x_min, y_min, f'{label.item()}: {score:.2f}', fontsize=12, color='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4840a57",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 15\n",
    "learning_rate =0.0008\n",
    "batch_size = 3\n",
    "val_size = 10\n",
    "train_size = 500\n",
    "\n",
    "val_err_array = np.array([])\n",
    "train_err_array = np.array([])\n",
    "nb_sample_array = np.array([])\n",
    "train_loss_classifier_array = np.array([])\n",
    "train_loss_objectness_array = np.array([])\n",
    "\n",
    "# Early stopping parameters\n",
    "patience =5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "full_train_dataset = (root=path + '/VOC2012_train_val/', image_set='train', transform=transform)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_indices = list(range(train_size))  # Indices for the first 1000 images\n",
    "val_indices = list(range(train_size, train_size + val_size))  # Next 100 images for validation\n",
    "train_subset = Subset(full_train_dataset, train_indices)\n",
    "val_subset = Subset(full_train_dataset, val_indices)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Load a pretrained Faster R-CNN model\n",
    "#model = detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "#model = detection.ssd300_vgg16(weights=\"DEFAULT\")\n",
    "model = detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Set the requires_grad attribute of all the backbone parameters to False\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Backbone frozen. Only the RPN and heads will be trained.\")\n",
    "# print(\"Backbone is not frozen and will be fine-tuned.\")\n",
    "\n",
    "\n",
    "# Modify the model for the number of classes\n",
    "num_classes = 21  # 20 classes + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "# Correct the import path for utils\n",
    "# in_channels = detection._utils.retrieve_out_channels(model.backbone, (300, 300))\n",
    "# num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "# model.head = detection.ssd.SSDHead(in_channels, num_anchors, num_classes)\n",
    "\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "#optimizer = optim.Adam(params, lr=learning_rate)\n",
    "optimizer = optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Function for validation\n",
    "def validate(model, val_loader):\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')  # Initialize best validation loss\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_loss_classifier = 0.0\n",
    "    epoch_loss_objectness = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    nb_used_sample = 0 # Initialize the number of samples used in this epoch\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        # Move images and targets to the device (GPU or CPU)\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Compute total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_loss += losses.item()\n",
    "        epoch_loss_classifier += loss_dict['loss_classifier'].item()\n",
    "        epoch_loss_objectness += loss_dict['loss_objectness'].item()\n",
    "        nb_used_sample += batch_size # Add the number of images in the current batch\n",
    "\n",
    "            # Calculate average training loss for the epoch\n",
    "    train_err = epoch_loss / len(train_loader)\n",
    "    train_loss_classifier = epoch_loss_classifier / len(train_loader)\n",
    "    train_loss_objectness = epoch_loss_objectness / len(train_loader)\n",
    "\n",
    "################ FOR VGG16 ###############\n",
    "    #     # Accumulate loss - Use keys appropriate for SSD\n",
    "    #     epoch_loss += losses.item()\n",
    "    #     # Assuming loss_dict for SSD contains 'classification' and 'bbox_regression'\n",
    "    #     if 'classification' in loss_dict:\n",
    "    #         epoch_loss_classifier += loss_dict['classification'].item()\n",
    "    #     if 'bbox_regression' in loss_dict:\n",
    "    #          epoch_loss_objectness += loss_dict['bbox_regression'].item() # Using objectness for regression loss here\n",
    "    #     nb_used_sample += batch_size\n",
    "\n",
    "\n",
    "    # # Calculate average training loss for the epoch\n",
    "    # train_err = epoch_loss / len(train_loader)\n",
    "    # # Calculate average for classifier and regression losses only if they were accumulated\n",
    "    # train_loss_classifier = epoch_loss_classifier / len(train_loader) if 'classification' in loss_dict else 0\n",
    "    # train_loss_objectness = epoch_loss_objectness / len(train_loader) if 'bbox_regression' in loss_dict else 0\n",
    "###########################################\n",
    "\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_err:.4f}, Classifier Loss: {train_loss_classifier:.4f}, Objectness Loss: {train_loss_objectness:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    val_loss = validate(model, val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    train_err_array = np.append(train_err_array, train_err)\n",
    "    val_err_array = np.append(val_err_array, val_loss)\n",
    "    nb_sample_array = np.append(nb_sample_array, nb_used_sample)\n",
    "    train_loss_classifier_array = np.append(train_loss_classifier_array, train_loss_classifier)\n",
    "    train_loss_objectness_array = np.append(train_loss_objectness_array, train_loss_objectness)\n",
    "\n",
    "\n",
    "    # Save the model weights if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'faster_rcnn_voc_best.pth')\n",
    "        print(f\"Model weights saved. New best validation loss: {best_val_loss:.4f}\")\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping after {patience} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Final message\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d1ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU et mAP qui viennent d'Object_Segmentation\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    # Slicing idx:idx+1 in order to keep tensor dimensionality\n",
    "    # Doing ... in indexing if there would be additional dimensions\n",
    "    # Like for Yolo algorithm which would have (N, S, S, 4) in shape\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Need clamp(0) in case they do not intersect, then we want intersection to be 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"corners\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
